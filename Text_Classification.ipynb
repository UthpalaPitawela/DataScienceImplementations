{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOsVJKbwicPj1LX/Hksrtrg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UthpalaPitawela/Data_Science_Implementations/blob/main/Text_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8duquAIHWbiZ"
      },
      "source": [
        "Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juRn38e1WfCH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee4c912d-4f83-4fad-859d-20343da5e35f"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "#libraries for pre processing\n",
        "from bs4 import BeautifulSoup\n",
        "!pip install contractions\n",
        "import contractions\n",
        "import re\n",
        "import string\n",
        "#to remove stop words\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import ToktokTokenizer\n",
        "tokenizer = ToktokTokenizer()\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "# custom: removing words from list\n",
        "stopword_list.remove('not')\n",
        "\n",
        "#for lemmatization\n",
        "import spacy\n",
        "nlp = spacy.load('en',parse=True,tag=True, entity=True)\n",
        "\n",
        "#libraries for feature extraction\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "NER = spacy.load(\"en_core_web_sm\")\n",
        "from nltk import pos_tag\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#libraries for evaluation metrics\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#libraries for SVM\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#for word embeddings\n",
        "!git clone https://github.com/facebookresearch/fastText.git\n",
        "!cd fastText\n",
        "!pip install fastText\n",
        "import fasttext.util\n",
        "fasttext.util.download_model('en', if_exists='ignore')  # English\n",
        "ft = fasttext.load_model('cc.en.300.bin')\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.0.52-py2.py3-none-any.whl (7.2 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.2.tar.gz (321 kB)\n",
            "\u001b[K     |████████████████████████████████| 321 kB 7.3 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.2.0-py3-none-any.whl (283 kB)\n",
            "\u001b[K     |████████████████████████████████| 283 kB 53.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85458 sha256=f1edebf9a0c42f140638c880f84ab24acf4841d38db97baef8d36a2eeadef0e6\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/19/a6/8f363d9939162782bb8439d886469756271abc01f76fbd790f\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.2.0 contractions-0.0.52 pyahocorasick-1.4.2 textsearch-0.0.21\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Cloning into 'fastText'...\n",
            "remote: Enumerating objects: 3854, done.\u001b[K\n",
            "remote: Total 3854 (delta 0), reused 0 (delta 0), pack-reused 3854\u001b[K\n",
            "Receiving objects: 100% (3854/3854), 8.22 MiB | 15.86 MiB/s, done.\n",
            "Resolving deltas: 100% (2417/2417), done.\n",
            "Collecting fastText\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 4.4 MB/s \n",
            "\u001b[?25hCollecting pybind11>=2.2\n",
            "  Using cached pybind11-2.7.1-py2.py3-none-any.whl (200 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fastText) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fastText) (1.19.5)\n",
            "Building wheels for collected packages: fastText\n",
            "  Building wheel for fastText (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fastText: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3090426 sha256=ae50ae791d0401f463687394232251ce01052a4984891f1236b44fbafb406ad7\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n",
            "Successfully built fastText\n",
            "Installing collected packages: pybind11, fastText\n",
            "Successfully installed fastText-0.9.2 pybind11-2.7.1\n",
            "Downloading https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9riK56uL93u"
      },
      "source": [
        "Data load from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WzwcqSoLDGS"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "file_id = '1FJYN3RvVIe_zb1XL8w7pbuXRv3S6O38S'\n",
        "downloaded = drive.CreateFile({'id': file_id})"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzV_s6kjUENd"
      },
      "source": [
        "# Download the file to a local disk as 'dataset.xlsx'.\n",
        "downloaded.GetContentFile('dataset.xlsx')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9AOSZXxUa1N",
        "outputId": "4b293d98-a640-4f76-bbf2-4fdf1d837294"
      },
      "source": [
        "# Here it is --\n",
        "!ls -lha dataset.xlsx"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 232K Aug 29 16:35 dataset.xlsx\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "8PX2cfkSUfAr",
        "outputId": "64e3146c-6880-4055-bf6a-8d0cede6810f"
      },
      "source": [
        "# Now, we can use pandas read_excel after installing the excel importer.\n",
        "!pip install -q xlrd\n",
        "\n",
        "import pandas as pd\n",
        "columns = ['question', 'coarse', 'fine']\n",
        "df = pd.read_excel('dataset.xlsx', header=None,names=columns)\n",
        "df"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>coarse</th>\n",
              "      <th>fine</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What are the special things we (husband and me...</td>\n",
              "      <td>TTD</td>\n",
              "      <td>TTDSIG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What are the companies which organize shark fe...</td>\n",
              "      <td>TTD</td>\n",
              "      <td>TTDOTH</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Is it safe for female traveller to go alone to...</td>\n",
              "      <td>TGU</td>\n",
              "      <td>TGUHEA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What are the best places around Cape Town for ...</td>\n",
              "      <td>TTD</td>\n",
              "      <td>TTDSIG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What are the best places to stay for a family ...</td>\n",
              "      <td>ACM</td>\n",
              "      <td>ACMOTH</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4995</th>\n",
              "      <td>What is the best area to be based for sightsee...</td>\n",
              "      <td>TTD</td>\n",
              "      <td>TTDSIG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4996</th>\n",
              "      <td>What are the good value traditional bars and r...</td>\n",
              "      <td>FOD</td>\n",
              "      <td>FODBAR</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4997</th>\n",
              "      <td>What are the hotels near Alicante bus station?</td>\n",
              "      <td>ACM</td>\n",
              "      <td>ACMHOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4998</th>\n",
              "      <td>Where to stay in La Gomera to mountain biking?</td>\n",
              "      <td>TTD</td>\n",
              "      <td>TTDSPO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4999</th>\n",
              "      <td>Is it possible to take a train trip from Santi...</td>\n",
              "      <td>TRS</td>\n",
              "      <td>TRSTRN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               question coarse    fine\n",
              "0     What are the special things we (husband and me...    TTD  TTDSIG\n",
              "1     What are the companies which organize shark fe...    TTD  TTDOTH\n",
              "2     Is it safe for female traveller to go alone to...    TGU  TGUHEA\n",
              "3     What are the best places around Cape Town for ...    TTD  TTDSIG\n",
              "4     What are the best places to stay for a family ...    ACM  ACMOTH\n",
              "...                                                 ...    ...     ...\n",
              "4995  What is the best area to be based for sightsee...    TTD  TTDSIG\n",
              "4996  What are the good value traditional bars and r...    FOD  FODBAR\n",
              "4997     What are the hotels near Alicante bus station?    ACM  ACMHOT\n",
              "4998     Where to stay in La Gomera to mountain biking?    TTD  TTDSPO\n",
              "4999  Is it possible to take a train trip from Santi...    TRS  TRSTRN\n",
              "\n",
              "[5000 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T445BRdoWBf"
      },
      "source": [
        "Pre processing\n",
        "\n",
        "\n",
        "*   Remove HTML tags\n",
        "*   Expand contractions\n",
        "*   Remove special cases\n",
        "*   Lowercase all texts\n",
        "*   Remove stop words\n",
        "*   Lemmatization\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yU-Ek0go6Ui"
      },
      "source": [
        "Remove HTML tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JddFXsHeo3sj",
        "outputId": "9cb58be7-6217-4d84-9891-c494336c7335"
      },
      "source": [
        "def strip_html_tags(text):\n",
        "    \"\"\"remove html tags from text\"\"\"\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    stripped_text = soup.get_text(separator=\" \")\n",
        "    return stripped_text\n",
        "\n",
        "\n",
        "df['html_removed_questions'] = [strip_html_tags(question) for question in df['question']]\n",
        "df['html_removed_questions'][1]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'What are the companies which organize shark feeding events for scuba divers?'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAHJlIOKbxTy"
      },
      "source": [
        "Expand contractions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-CMCMLldeih",
        "outputId": "59a774fa-c2c8-429f-89db-53c23f9494fa"
      },
      "source": [
        "def expand_contractions(text):\n",
        "    \"\"\"expand shortened words, e.g. don't to do not\"\"\"\n",
        "    text = contractions.fix(text)\n",
        "    return text\n",
        "\n",
        "item = \"What are the best tourist excursions when it's raining?\";\n",
        "\n",
        "df['contractions_removed_questions'] = [expand_contractions(question) for question in df['html_removed_questions']]\n",
        "print(item);\n",
        "print(expand_contractions(item));\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What are the best tourist excursions when it's raining?\n",
            "What are the best tourist excursions when it is raining?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xop8npTZf0X3"
      },
      "source": [
        "Remove special cases and punctuation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2SDgT5-fz13",
        "outputId": "c13d5b21-635e-4162-d7ba-3f8afff0880e"
      },
      "source": [
        "# function to remove special characters\n",
        "def remove_special_characters_punctuation(text):\n",
        "    # define the pattern to keep\n",
        "    pat = r'[^a-zA-z.,!?/:;\\\"\\'\\s\\n]' \n",
        "    formattedText = re.sub(pat, '', text)\n",
        "    text = ''.join([c for c in formattedText if c not in string.punctuation])\n",
        "    return text\n",
        "    \n",
        "item = \"What are the best tourist excursions when it's raining?\";\n",
        "\n",
        "df['special_characters_removed_questions'] = [remove_special_characters_punctuation(question) for question in df['contractions_removed_questions']]\n",
        "print(item);\n",
        "print(remove_special_characters_punctuation(item));"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What are the best tourist excursions when it's raining?\n",
            "What are the best tourist excursions when its raining\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYxOkpqbiOQ3"
      },
      "source": [
        "Lower case text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-tWdzNeidmF",
        "outputId": "5464a07d-fd7c-4325-9cf7-6f941010574b"
      },
      "source": [
        "def lower_case_text(text):\n",
        "  text = text.strip().lower()\n",
        "  return text;\n",
        "\n",
        "item = \"Does anyone have the e-mail address for the Ambre resort & spa Mauritius?\";\n",
        "\n",
        "df['lower_cased_questions'] = [lower_case_text(question) for question in df['special_characters_removed_questions']]\n",
        "print(item);\n",
        "print(lower_case_text(item));\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Does anyone have the e-mail address for the Ambre resort & spa Mauritius?\n",
            "does anyone have the e-mail address for the ambre resort & spa mauritius?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcY_rsgri5oQ"
      },
      "source": [
        "Remove stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2dTc7Eai9FH",
        "outputId": "eba608ad-0e9d-4e24-98b8-0e800f90c6ae"
      },
      "source": [
        "# function to remove stopwords\n",
        "def remove_stopwords(text):\n",
        "    # convert sentence into token of words\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    # check in lowercase \n",
        "    t = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    text = ' '.join(t)    \n",
        "    return text\n",
        "\n",
        "item = \"What is the ideal time to visit Mauritius in the year as we are planning to get married in there and spend our honeymoon?\";\n",
        "df['stop_words_removed_questions'] = [remove_stopwords(question) for question in df['lower_cased_questions']]\n",
        "print(item);\n",
        "print(remove_stopwords(item));"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What is the ideal time to visit Mauritius in the year as we are planning to get married in there and spend our honeymoon?\n",
            "ideal time visit Mauritius year planning get married spend honeymoon ?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZGS0L8NHb_y"
      },
      "source": [
        "Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0iuVzJckK_h",
        "outputId": "854f1a04-1087-4656-f577-c9d976cdb371"
      },
      "source": [
        "def get_lem(text):\n",
        "    text = nlp(text)\n",
        "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
        "    return text\n",
        "  \n",
        "item = \"What is the ideal time to visit Mauritius in the year as we are planning to get married in there and spend our honeymoon?\";\n",
        "df['lemmatized_questions'] = [get_lem(question) for question in df['stop_words_removed_questions']]\n",
        "print(item);\n",
        "print(get_lem(item));"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What is the ideal time to visit Mauritius in the year as we are planning to get married in there and spend our honeymoon?\n",
            "what be the ideal time to visit Mauritius in the year as we be plan to get marry in there and spend our honeymoon ?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SB3YguYqkITK"
      },
      "source": [
        "Create Features\n",
        "\n",
        "*   POS tags\n",
        "*   Named Entities\n",
        "*   Head Word Synonyms\n",
        "*   Bi-grams\n",
        "*   Head words\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvZRCJHakH-w"
      },
      "source": [
        "POS Tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOHiVS2kSWEO",
        "outputId": "a92c8349-0136-493a-dc77-d7038b78d727"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "def pos_tagging(text):\n",
        "    words = nltk.word_tokenize(text)\n",
        "    lst = [ r[1] for r in pos_tag(words)] \n",
        "    return ' '.join(lst)\n",
        "\n",
        "\n",
        "item = \"What is the ideal time to visit Mauritius in the year as we are planning to get married in there and spend our honeymoon?\";\n",
        "df['pos_tagged_questions'] = [pos_tagging(question) for question in df['stop_words_removed_questions']]\n",
        "print(pos_tagging(item));\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "WP VBZ DT JJ NN TO VB NNP IN DT NN IN PRP VBP VBG TO VB VBN IN EX CC VB PRP$ NN .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78v-dqAR-ze3"
      },
      "source": [
        "Named Entities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNOg1K17K6EA",
        "outputId": "a16e8a49-5617-4488-dd74-b65b9240dfdd"
      },
      "source": [
        "def getNamedEntities(text):\n",
        "  NER_text = NER(text);\n",
        "  NER_chunk = [];\n",
        "  for word in NER_text.ents:\n",
        "    label  = word.label_;\n",
        "    if label not in NER_chunk:\n",
        "      NER_chunk.append(word.label_)\n",
        "  def remove_null(x):\n",
        "        if '' in x:\n",
        "            x.remove('')\n",
        "        return x\n",
        "  lst = remove_null(NER_chunk)\n",
        "  # if len(lst)>0:\n",
        "  #   NER_data = lst[0]\n",
        "  #   for i in lst[1:]:\n",
        "  #       NER_data = NER_data + ' ' + i\n",
        "    # remove_null(NER_data)\n",
        "  return ' '.join(lst)\n",
        "  \n",
        "item = \"What is the ideal time to visit Mauritius in the year as we are planning to get married in there and spend our honeymoon?\";\n",
        "df['named_entity_questions'] = [getNamedEntities(question) for question in df['stop_words_removed_questions']]\n",
        "print(getNamedEntities(item));\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPE DATE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vXJO7YhlkJr"
      },
      "source": [
        "Count vectorizer (BOW)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fI8N19wfllec",
        "outputId": "d32e424d-58bb-47e8-f234-ac14af2f1d09"
      },
      "source": [
        "def get_count_vect(documents):\n",
        "    vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7)#, stop_words=stopwords.words('english')\n",
        "    X = vectorizer.fit_transform(documents).toarray()\n",
        "    print(X.shape)\n",
        "    return X\n",
        "\n",
        "get_count_vect(df['stop_words_removed_questions'])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5000, 1072)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A61URMarm4Gj"
      },
      "source": [
        "Head word Synonyms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFOgrpTlm747",
        "outputId": "fffb2676-38c0-48d7-927b-57ba57e6e37f"
      },
      "source": [
        "from nltk.corpus import wordnet \n",
        "nltk.download('wordnet')\n",
        "def get_synonyms(words):\n",
        "    all_synonyms = []\n",
        "    for word in words.split(' '):\n",
        "        synonyms = []\n",
        "\n",
        "        for syn in wordnet.synsets(word):\n",
        "            for l in syn.lemmas():\n",
        "                synonyms.append(l.name())\n",
        "\n",
        "        synonyms = list(set(synonyms))\n",
        "        all_synonyms += synonyms\n",
        "        \n",
        "    return all_synonyms\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHwCddCTnekd",
        "outputId": "e3b1eec7-aadb-475f-a9af-335e2d919999"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "head_words_synonym_vectorizer = CountVectorizer(tokenizer = get_synonyms,max_features=100,stop_words=stopwords.words('english'))\n",
        "head_words_synonym_vector = head_words_synonym_vectorizer.fit_transform(df['stop_words_removed_questions'].values).toarray()\n",
        "print(head_words_synonym_vector)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'tween\", '1', '1000', '500', 'A', 'AM', 'AN', 'AS', 'Am', 'American_Samoa', 'Artium_Magister', 'As', 'Associate_in_Nursing', 'At', 'Bay_State', 'Be', 'Beaver_State', 'D', 'DO', 'DOE', 'Department_of_Energy', 'Doctor_of_Osteopathy', 'Don', 'Don_River', 'Down', 'Eastern_Samoa', 'Energy', 'Energy_Department', 'Evergreen_State', 'G', 'HA', 'He', 'Hera', 'Here', 'Hoosier_State', 'I', 'IN', 'ISN', 'IT', 'In', 'Indiana', 'International_Relations_and_Security_Network', 'John_L._H._Down', 'K', \"KO'd\", 'Lapp', 'Lapplander', 'M', 'MA', 'MB', 'ME', 'MT', 'Maine', 'Massachusetts', 'Master_of_Arts', 'MiB', 'More', 'No', 'North_Korean_won', 'O', 'OR', 'Old_Colony', 'Oregon', 'Pine_Tree_State', 'Ra', 'Re', 'S', 'Saame', 'Saami', 'Same', 'Sami', 'Shan', 'Sir_Thomas_More', 'South_Korean_won', 'T', 'Tai_Long', 'Thomas_More', 'WA', 'WHO', 'Washington', 'World_Health_Organization', 'Y', 'accept', 'ace', 'acquire', 'act', 'adenine', 'advance', 'afterward', 'afterwards', 'ahead', 'all_over', 'almost', 'alone', 'along', 'also', 'altogether', 'americium', 'amp', 'ampere', 'amplitude_modulation', 'and_so', 'and_then', 'ane', 'angstrom', 'angstrom_unit', 'answer', 'antiophthalmic_factor', 'apiece', 'approximately', 'ar', 'around', 'arrange', 'arse', 'arsenic', 'as_well', 'aside', 'ass', 'assume', 'astatine', 'astir', 'at_a_lower_place', 'at_once', 'at_one_time', 'at_present', 'at_that_place', 'atomic_number_102', 'atomic_number_16', 'atomic_number_2', 'atomic_number_33', 'atomic_number_39', 'atomic_number_4', 'atomic_number_49', 'atomic_number_53', 'atomic_number_75', 'atomic_number_8', 'atomic_number_85', 'atomic_number_95', 'away', 'axerophthol', 'backside', 'barely', 'bash', 'bathroom', 'bear', 'behave', 'behind', 'beingness', 'belt_down', 'beneath', 'bequeath', 'beryllium', 'besides', 'betwixt', 'birth', 'blue', 'bolt_down', 'boost', 'bottom', 'brawl', 'bring_home_the_bacon', 'bum', 'bump_off', 'buns', 'butt', 'buttocks', 'calciferol', 'can_buoy', 'cancelled', 'canful', 'cause', 'chiliad', 'cholecalciferol', 'close_to', 'coif', 'coiffe', 'coiffure', 'come', 'come_out', 'come_out_of_the_closet', 'come_through', 'commode', 'complete', 'completely', 'comprise', 'concluded', 'constitute', 'consume', 'cost', 'crapper', 'cut_down', 'deliver', 'deliver_the_goods', 'deoxyadenosine_monophosphate', 'deoxythymidine_monophosphate', 'depressed', 'derriere', 'devour', 'directly', 'dismiss', 'dispatch', 'dispirited', 'displace', 'doe', 'doh', 'done', 'down_feather', 'down_in_the_mouth', 'down_pat', 'down_the_stairs', 'downcast', 'downhearted', 'downstairs', 'downward', 'downwardly', 'downwards', 'dress', 'drink_down', 'due_south', 'earlier', 'early', 'embody', 'encourage', 'ended', 'entirely', 'entropy', 'equal', 'equally', 'equitable', 'ergocalciferol', 'erst', 'erstwhile', 'every_bit', 'exactly', 'excessively', 'exclusively', 'execute', 'exercise', 'exist', 'existence', 'experience', 'extinct', 'fair', 'fanny', 'far', 'fare', 'farther', 'father', 'feature', 'fine-tune', 'fire', 'five_hundred', 'follow', 'for_each_one', 'forbidden', 'force_out', 'former', 'formerly', 'forth', 'forthwith', 'foster', 'from_each_one', 'fundament', 'gain', 'gain_ground', 'get', 'get_ahead', 'get_along', 'get_into', 'give', 'give_birth', 'give_notice', 'give_the_axe', 'give_the_sack', 'gloomy', 'glucinium', 'go_through', 'good', 'grand', 'grim', 'group_A', 'group_O', 'harbor', 'harbour', 'hardly', 'have_got', 'helium', 'hence', 'higher_up', 'hind_end', 'hindquarters', 'hit', 'hither', 'hold', 'hour_angle', 'identical', 'immediately', 'improving', 'in_a_higher_place', 'in_front', 'in_one_case', 'in_that_location', 'in_that_respect', 'inch', 'indeed', 'indium', 'induce', 'information_technology', 'infra', 'ingest', 'instantly', 'inward', 'inwards', 'iodin', 'iodine', 'john', 'just_about', 'just_now', 'k', 'kayoed', 'keister', 'kill', 'knock_down', 'knocked_out', 'land', 'later', 'later_on', 'lav', 'lavatory', 'leave', 'let', 'like', 'like_a_shot', 'likewise', 'liothyronine', 'live', 'lone', 'lonesome', 'low', 'low-spirited', 'mA', 'make', 'make_headway', 'make_out', 'make_up', 'mama', 'mamma', 'mammy', 'manage', 'mastered', 'mebibyte', 'megabyte', 'merely', 'meter', 'metre', 'metric_ton', 'mho', 'milliampere', 'molar_concentration', 'molarity', 'mom', 'momma', 'mommy', 'more_or_less', 'more_than', 'mum', 'mummy', 'murder', 'nates', 'near', 'nearly', 'nether', 'nigh', 'no_more', 'nobelium', 'non', 'nowadays', \"o'er\", 'oasis', 'on_a_lower_floor', 'on_that_point', 'once_again', 'once_more', 'one', 'one_thousand', 'one_time', 'only_if', 'only_when', 'operating_room', 'operating_theater', 'operating_theatre', 'or_so', 'organism', 'over_again', 'overly', 'oxygen', 'past', 'patch', 'perform', 'personify', 'piece', 'pile', 'polish', 'polish_off', 'pop', 'possess', 'posterior', 'pot', 'potty', 'pour_down', 'practice', 'practise', 'prat', 'preceptor', 'precisely', 'privy', 'prohibited', 'promote', 'proscribed', 'pull_ahead', 'pull_down', 'push_down', 'put_on', 'put_up', 'randomness', 'rattling', 'ray', 'real', 'really', 'rear', 'rear_end', 'receive', 'reciprocal_ohm', 'refine', 'remove', 'represent', 'rhenium', 'rich_person', 'right_away', 'roughly', 'rump', 'sack', 'scarce', 'scarcely', 'seaport', 'seat', 'sec', 'second', 'selfsame', 'send_away', 'serve', 'set', 'shoot_down', 'siemens', 'simply', 'single', 'slay', 'soh', 'sol', 'sole', 'solely', 'solitary', 'sour', 'south', 'southward', 'spell', 'stern', 'stimulate', 'stool', 'straight_off', 'straightaway', 'stunned', 'subsequently', 'succeed', 'suffer', 'suffice', 'sulfur', 'sulphur', 'supra', 'surgery', 'sustain', 'taboo', 'tabu', 'tail', 'tail_end', 'take', 'take_in', 'terminate', 'terminated', 'testament', 'tetraiodothyronine', 'thence', 'therefore', 'thither', 'thou', 'thousand', 'throne', 'through_and_through', 'through_with', 'throw', 'thus', 'thusly', 'thymine', 'thyroxin', 'thyroxine', 'tin', 'tin_can', 'to_a_fault', 'to_a_greater_extent', 'to_a_higher_place', 'to_a_lower_place', 'to_each_one', 'to_the_highest_degree', 'today', 'toilet', 'tonne', 'tooshie', 'toss_off', 'totally', 'triiodothyronine', 'turned', 'tush', 'type_A', 'type_O', 'unity', 'upright', 'upward', 'upwardly', 'upwards', 'ut', 'verboten', 'viosterol', 'virtually', 'vitamin_A', 'vitamin_D', 'volition', 'wealthy_person', 'wear', 'well-nigh', 'whatever', 'whatsoever', 'wherefore', 'whole', 'wholly', 'win', 'wye', 'yard', 'yttrium'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlX3NyL_ElNA"
      },
      "source": [
        "Head Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3_38z2SEnAt"
      },
      "source": [
        "# Head word tokenizer\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "def head_word_tokenizer(text):\n",
        "    head_words = []\n",
        "    for token in nlp(text):\n",
        "        if token.dep_ == \"nsubj\" or token.dep_ == \"nsubjpass\":\n",
        "            head_words.append(token.text)\n",
        "#             head_words.append(token.head.text)\n",
        "    return head_words"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMnfBTi7EvIM",
        "outputId": "aeb34c6d-ca92-4176-c249-24096a6f2a13"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "head_words_vectorizer = CountVectorizer(tokenizer = head_word_tokenizer,max_features=100,stop_words=stopwords.words('english'))\n",
        "head_words_vector = head_words_vectorizer.fit_transform(df[\"stop_words_removed_questions\"].values).toarray()\n",
        "print(head_words_vector)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DkZ3a4y4Onu"
      },
      "source": [
        "Bi-grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tOMTqZW4QNc",
        "outputId": "24d48513-adce-4669-9342-2dbd9640eb6d"
      },
      "source": [
        "def bigram(text):\n",
        "    new_words = \"\"\n",
        "    pre_word = None\n",
        "    for word in text.strip().split(' '):\n",
        "        \n",
        "        if pre_word is not None:\n",
        "            new_words += \"{}{} \".format(pre_word, word)\n",
        "        pre_word = word\n",
        "    return new_words[:-1]\n",
        "\n",
        "# print(bigram(\"what are the company which organize shark feeding event for scuba divers \"))\n",
        "\n",
        "item = \"What is the ideal time to visit Mauritius in the year as we are planning to get married in there and spend our honeymoon?\";\n",
        "df['bigram_questions'] = [bigram(question) for question in df['stop_words_removed_questions']]\n",
        "\n",
        "print(bigram(item))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Whatis isthe theideal idealtime timeto tovisit visitMauritius Mauritiusin inthe theyear yearas aswe weare areplanning planningto toget getmarried marriedin inthere thereand andspend spendour ourhoneymoon?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouY2ch1v6XKt"
      },
      "source": [
        "Define course class values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mt-2ckW76hM7"
      },
      "source": [
        "def get_class_values(feature_set='coarse'):\n",
        "    le = LabelEncoder()\n",
        "    encoded_classes = le.fit_transform(df[feature_set])\n",
        "    return encoded_classes"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ru3murev7Y7b"
      },
      "source": [
        "Define evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJLd_dsI7cU9"
      },
      "source": [
        "def PRC_matrics(y_test, prediction):\n",
        "    # prediction\n",
        "    precision = precision_score(y_test, prediction, labels=np.unique(prediction), average='micro')*100\n",
        "    print('Precision: %.3f' % precision)\n",
        "\n",
        "    # recall\n",
        "    recall = recall_score(y_test, prediction, labels=np.unique(prediction), average='micro')*100\n",
        "    print('Recall: %.3f' % recall)\n",
        "    \n",
        "    # score\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "    print('F-Measure: %.3f' % f1_score)\n",
        "    \n",
        "    \n",
        "    acc = accuracy_score(y_test, prediction)*100\n",
        "    print('Accuracy score: %.3f' % acc)\n",
        "    \n",
        "    \n",
        "    cm = confusion_matrix(y_test, prediction)\n",
        "    print(\"\\nConfustion matrix: \\n{}\".format(cm))\n",
        "    \n",
        "    return precision, recall, f1_score, acc"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itPRPb8o6rKY"
      },
      "source": [
        "Define SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGxE4Ba36tL2"
      },
      "source": [
        "def train_with_svm(XX, y):\n",
        "    best_prediction = None\n",
        "    best_test = None\n",
        "    best_accuracy = 0\n",
        "    cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
        "    fold = 0\n",
        "    accuracies = []\n",
        "    for train_index, test_index in cv.split(XX):\n",
        "        fold += 1\n",
        "        X_train, X_test = XX[train_index], XX[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "        SVM = SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
        "        SVM.fit(X_train,y_train)\n",
        "        predictions_SVM1 = SVM.predict(X_test)\n",
        "        acc = accuracy_score(predictions_SVM1, y_test)*100\n",
        "        if best_accuracy < acc:\n",
        "            best_accuracy = acc\n",
        "            best_prediction = predictions_SVM1\n",
        "            best_test = y_test\n",
        "            best_model = SVM\n",
        "        accuracies.append(acc)\n",
        "        print(\"Fold - {} - {} - {:.2f}\".format(fold, \"Accuracy-> \",acc))\n",
        "\n",
        "    print(\"Mean Accuracy {:.2f} \\nStd Accuracy {:.2f}\\n\\n\".format(np.mean(accuracies), np.std(accuracies)))\n",
        "    \n",
        "    print(\"Best accuracy : {}\".format(best_accuracy))\n",
        "    PRC_matrics(best_test, best_prediction)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFSq7ShR74F2"
      },
      "source": [
        "**Question 01: A traditional ML classifier s.a. SVM or Logistic Regression with at least 5  of the features mentioned in the paper**\n",
        "\n",
        "Selected Features\n",
        "\n",
        "*   POS tags\n",
        "*   Named Entities\n",
        "*   Head words\n",
        "*   Head Word Synonyms\n",
        "*   Bi-grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGaA6yX9-Wkt",
        "outputId": "1c95bb6a-83ac-4854-ba9a-1967e8c137bc"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from scipy.sparse import coo_matrix, csr_matrix, hstack\n",
        "\n",
        "\n",
        "coarse = get_class_values('coarse')\n",
        "\n",
        "X_lem = df['lemmatized_questions']\n",
        "tfidf_lem = TfidfVectorizer(max_features=5000)\n",
        "tfidf_lem.fit(X_lem)\n",
        "\n",
        "X_pos = df['pos_tagged_questions']\n",
        "\n",
        "X_ne = df['named_entity_questions']\n",
        "\n",
        "X_bigram = df['bigram_questions']\n",
        "\n",
        "\n",
        "XX = csr_matrix(hstack([tfidf_lem.transform(X_lem) ,get_count_vect(X_pos), get_count_vect(X_ne), get_count_vect(X_bigram),\n",
        "                        head_words_vector,head_words_synonym_vector]))\n",
        "XX.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5000, 22)\n",
            "(5000, 13)\n",
            "(5000, 344)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 5561)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSH7d5xbeGBz"
      },
      "source": [
        "Feature Selection\n",
        "\n",
        "1. Select k best features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pekOB2TReL2B",
        "outputId": "692c7296-626a-46f7-98b4-4044847d3558"
      },
      "source": [
        "\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "X_new = SelectKBest(chi2, k=3500).fit_transform(XX, coarse)\n",
        "X_new.shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 3500)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLZqGivvemQH"
      },
      "source": [
        "2. Remove features considering a specified threshold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5zn2hvxep1P",
        "outputId": "99d16c6a-d66f-4acd-97e4-770c5eebb4db"
      },
      "source": [
        "from sklearn.feature_selection import VarianceThreshold\n",
        "X_new_var = VarianceThreshold(threshold=(0.01)).fit_transform(XX)\n",
        "X_new_var.shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 132)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlRc_-JbfC7F"
      },
      "source": [
        "Training using the defined SVM\n",
        "\n",
        "1. Train with coarse classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLcy-jyMfFWB",
        "outputId": "43327da8-a703-490f-fd18-25c883510dd8"
      },
      "source": [
        "train_with_svm(XX, get_class_values('coarse'))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold - 1 - Accuracy->  - 81.60\n",
            "Fold - 2 - Accuracy->  - 78.60\n",
            "Fold - 3 - Accuracy->  - 76.00\n",
            "Fold - 4 - Accuracy->  - 77.80\n",
            "Fold - 5 - Accuracy->  - 79.40\n",
            "Fold - 6 - Accuracy->  - 78.40\n",
            "Fold - 7 - Accuracy->  - 76.00\n",
            "Fold - 8 - Accuracy->  - 77.00\n",
            "Fold - 9 - Accuracy->  - 78.80\n",
            "Fold - 10 - Accuracy->  - 78.40\n",
            "Mean Accuracy 78.20 \n",
            "Std Accuracy 1.58\n",
            "\n",
            "\n",
            "Best accuracy : 81.6\n",
            "Precision: 81.600\n",
            "Recall: 81.600\n",
            "F-Measure: 81.600\n",
            "Accuracy score: 81.600\n",
            "\n",
            "Confustion matrix: \n",
            "[[ 69   1   3   2   2   4   0]\n",
            " [  0  14   2   0   2   7   0]\n",
            " [  4   2  41   5   0   4   0]\n",
            " [  4   0   1  86   6   9   0]\n",
            " [  2   0   2   4  85   3   0]\n",
            " [  5   3   0   7   3 100   0]\n",
            " [  0   0   0   2   0   3  13]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9Vdql3zgGy4"
      },
      "source": [
        "2. Train with fine classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RL-iL6YOgJ7B",
        "outputId": "e2d08135-adf7-4a0a-f42a-778d1301b0d0"
      },
      "source": [
        "train_with_svm(XX, get_class_values('fine'))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold - 1 - SVM Accuracy Score ->  - 52.60\n",
            "Fold - 2 - SVM Accuracy Score ->  - 51.40\n",
            "Fold - 3 - SVM Accuracy Score ->  - 49.40\n",
            "Fold - 4 - SVM Accuracy Score ->  - 52.60\n",
            "Fold - 5 - SVM Accuracy Score ->  - 52.20\n",
            "Fold - 6 - SVM Accuracy Score ->  - 51.20\n",
            "Fold - 7 - SVM Accuracy Score ->  - 50.40\n",
            "Fold - 8 - SVM Accuracy Score ->  - 51.20\n",
            "Fold - 9 - SVM Accuracy Score ->  - 53.40\n",
            "Fold - 10 - SVM Accuracy Score ->  - 52.00\n",
            "Mean Accuracy 51.64 \n",
            "Std Accuracy 1.11\n",
            "\n",
            "\n",
            "Best accuracy : 53.400000000000006\n",
            "Precision: 53.400\n",
            "Recall: 55.394\n",
            "F-Measure: 54.379\n",
            "Accuracy score: 53.400\n",
            "\n",
            "Confustion matrix: \n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 1 0 ... 0 0 0]\n",
            " [0 0 1 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 7 0 0]\n",
            " [0 0 0 ... 1 2 0]\n",
            " [0 0 0 ... 1 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RHXeYbikDIB"
      },
      "source": [
        "**Question 02: 2. A traditional ML classifier s.a. SVM or Logistic Regression with word embedding features s.a. word2vec or fasttext.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAq7dudgoEOc"
      },
      "source": [
        "Word Embeddings: Using fasttext"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB795EZVzWNv"
      },
      "source": [
        "def get_embedding(sentence):    \n",
        "    embedding=ft.get_sentence_vector(sentence)\n",
        "    return embedding"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXQRx6h1ojXm"
      },
      "source": [
        "X_embedding = np.array([get_embedding(question) for question in df['stop_words_removed_questions'].values])\n",
        "le = LabelEncoder()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93BOToFOt1C5"
      },
      "source": [
        "Train with SVM for coarse classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ma5tXPZ7uHNU",
        "outputId": "a1df6419-b24f-47d2-f9e9-1a412b255c84"
      },
      "source": [
        "train_with_svm(X_embedding, get_class_values('coarse'))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold - 1 - SVM Accuracy Score ->  - 79.60\n",
            "Fold - 2 - SVM Accuracy Score ->  - 79.20\n",
            "Fold - 3 - SVM Accuracy Score ->  - 73.20\n",
            "Fold - 4 - SVM Accuracy Score ->  - 78.00\n",
            "Fold - 5 - SVM Accuracy Score ->  - 79.00\n",
            "Fold - 6 - SVM Accuracy Score ->  - 79.00\n",
            "Fold - 7 - SVM Accuracy Score ->  - 76.60\n",
            "Fold - 8 - SVM Accuracy Score ->  - 78.20\n",
            "Fold - 9 - SVM Accuracy Score ->  - 80.20\n",
            "Fold - 10 - SVM Accuracy Score ->  - 80.20\n",
            "Mean Accuracy 78.32 \n",
            "Std Accuracy 1.99\n",
            "\n",
            "\n",
            "Best accuracy : 80.2\n",
            "Precision: 80.200\n",
            "Recall: 80.522\n",
            "F-Measure: 80.361\n",
            "Accuracy score: 80.200\n",
            "\n",
            "Confustion matrix: \n",
            "[[  0   0   0   0   0   0   0   1   0]\n",
            " [  0  54   0   2   5   0   2   7   1]\n",
            " [  0   1  10   1   0   0   0   5   0]\n",
            " [  0   2   2  46   1   0   1   2   0]\n",
            " [  0   3   0   3  98   0   4  18   0]\n",
            " [  0   0   0   0   1   0   0   0   0]\n",
            " [  0   1   1   0   6   0  80   7   0]\n",
            " [  0   4   2   1   9   0   3 101   1]\n",
            " [  0   0   0   0   1   0   0   1  12]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9H5tZBYupBQ"
      },
      "source": [
        "Train with SVM for fine classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikb_3JlauvN-",
        "outputId": "919cc406-4bbc-4379-db89-2a41891b4c38"
      },
      "source": [
        "train_with_svm(X_embedding, get_class_values('fine'))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold - 1 - SVM Accuracy Score ->  - 54.40\n",
            "Fold - 2 - SVM Accuracy Score ->  - 56.00\n",
            "Fold - 3 - SVM Accuracy Score ->  - 54.80\n",
            "Fold - 4 - SVM Accuracy Score ->  - 52.00\n",
            "Fold - 5 - SVM Accuracy Score ->  - 53.40\n",
            "Fold - 6 - SVM Accuracy Score ->  - 55.40\n",
            "Fold - 7 - SVM Accuracy Score ->  - 54.20\n",
            "Fold - 8 - SVM Accuracy Score ->  - 54.20\n",
            "Fold - 9 - SVM Accuracy Score ->  - 57.00\n",
            "Fold - 10 - SVM Accuracy Score ->  - 53.40\n",
            "Mean Accuracy 54.48 \n",
            "Std Accuracy 1.35\n",
            "\n",
            "\n",
            "Best accuracy : 56.99999999999999\n",
            "Precision: 57.000\n",
            "Recall: 67.059\n",
            "F-Measure: 61.622\n",
            "Accuracy score: 57.000\n",
            "\n",
            "Confustion matrix: \n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 6 0 0]\n",
            " [0 0 0 ... 3 0 0]\n",
            " [0 0 0 ... 2 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT6HlYQMyTt0"
      },
      "source": [
        "**Question 03: A NN classifier s.a. an LSTM for classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqEqEHj0yZzX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}